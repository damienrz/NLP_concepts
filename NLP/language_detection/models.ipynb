{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This notebook was inspired by an interview question I was asked recently. The question was how I would go about detecting what language a website is written in. Sometimes the language data is simply located in the html, but I want to see if I could train a model that can detect the language by looking at sample sentences. Using the data I scraped from Wikipedia, I'm going to try training a couple different models and see how each of them perform. The goal is to determine the language a given sentence is written in with as high an accuracy as possible"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read dataframe\n",
    "language_df = pd.read_pickle('language_data.pickle')\n",
    "\n",
    "# shuffle dataframe\n",
    "language_df = language_df.sample(frac=1).reset_index(drop=True)\n",
    "language_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>language_code</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>Alice eats them, and they reduce her again in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[alice, eat, reduce, size]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de</td>\n",
       "      <td>Alice’s Abenteuer im Wunderland Filmhörspiele ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[alice, ’s, abenteuer, wunderland, filmhörspie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ja</td>\n",
       "      <td>『アリス』の注釈者 は、アリスの物語は（「あらゆる偉大な空想物語と同様に」）どんな象徴的解釈...</td>\n",
       "      <td>1</td>\n",
       "      <td>[アリス, 注釈, 者, アリス, 物語, あらゆる, 偉大, 空想, 物語, 同様, どん...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it</td>\n",
       "      <td>–\"Alice nel Paese delle Meraviglie\" rimanda qui.</td>\n",
       "      <td>6</td>\n",
       "      <td>[alice, meraviglie, rimandare]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zh</td>\n",
       "      <td>愛麗絲觀察著這個過程，並跟蛙先生講了一堆晦澀難懂的話，最後還是讓自己走進屋內</td>\n",
       "      <td>7</td>\n",
       "      <td>[愛麗, 絲, 觀察, 著這, 過程, 並, 跟, 蛙, 先生, 講, 了, 晦澀, 難懂,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                           sentence  language_code  \\\n",
       "0       en  Alice eats them, and they reduce her again in ...              0   \n",
       "1       de  Alice’s Abenteuer im Wunderland Filmhörspiele ...              3   \n",
       "2       ja  『アリス』の注釈者 は、アリスの物語は（「あらゆる偉大な空想物語と同様に」）どんな象徴的解釈...              1   \n",
       "3       it   –\"Alice nel Paese delle Meraviglie\" rimanda qui.              6   \n",
       "4       zh             愛麗絲觀察著這個過程，並跟蛙先生講了一堆晦澀難懂的話，最後還是讓自己走進屋內              7   \n",
       "\n",
       "                                              tokens  \n",
       "0                         [alice, eat, reduce, size]  \n",
       "1  [alice, ’s, abenteuer, wunderland, filmhörspie...  \n",
       "2  [アリス, 注釈, 者, アリス, 物語, あらゆる, 偉大, 空想, 物語, 同様, どん...  \n",
       "3                     [alice, meraviglie, rimandare]  \n",
       "4  [愛麗, 絲, 觀察, 著這, 過程, 並, 跟, 蛙, 先生, 講, 了, 晦澀, 難懂,...  "
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LangDetect Library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Before I train anything I want to see how accurate the langdetect library would be on this data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from collections import Counter\n",
    "from langdetect import detect as ld\n",
    "\n",
    "def get_lang(text):\n",
    "    try:\n",
    "        return ld(text)\n",
    "    except:\n",
    "        return 'fail'\n",
    "\n",
    "accuracy_list = []\n",
    "lang_targets = language_df['language'].tolist()\n",
    "ld_preds = language_df['sentence'].apply(lambda x: get_lang(x)).tolist()\n",
    "for t, p in zip(lang_targets, ld_preds):\n",
    "    accuracy_list.append(t==p)\n",
    "\n",
    "counts = Counter(accuracy_list)\n",
    "print(\"Accuracy:\", counts[True]/len(accuracy_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.8563563563563563\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 86% isn't bad; the accuracy may be affected by the quality of the data and not necessarily because of the langetect library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BOW Feature Extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Since this model's going to be trained to classify languages, I don't think that sequence will play a critical role. The uniqueness of the words in each language should be enough for a model to learn how to make the correct distinctions. If that's the case then a Bag of Words model should be all I need, so I'll start by creating a count vectorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tokens = language_df['tokens'].values\n",
    "y = language_df['language_code'].values\n",
    "#y = language_df['language_vector].tolist() # for one-hot-encoding\n",
    "\n",
    "# disable analyzer since I'm applying CountVectorizer to list of lemms\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# K-Fold Cross Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### There are many different classification models to choose from. Naturally I want to choose the model with the highest accuracy. To accomplish this I'll use ten fold cross validation to compare the accuracies between the Support Vector Machine, Multinomial Naive Bayes, and Random Forest models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# classification models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [SVC(), MultinomialNB(), RandomForestClassifier()]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def get_score(model, vectorizer):\n",
    "    clf = Pipeline([('vect', vectorizer), ('model', model)])\n",
    "    return cross_val_score(clf, tokens, y, cv=10) # ndarray\n",
    "\n",
    "for model in models:\n",
    "    scores = get_score(model, vectorizer)\n",
    "    print(f'{model}: {scores.mean()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVC(): 0.7717713567839196\n",
      "MultinomialNB(): 0.9499497487437185\n",
      "RandomForestClassifier(): 0.8293391959798996\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes outperforms the other two models! And it's average accuracy is pretty high. Similar to cross validation, for the actual model I'll use a tenth of the data for testing and the rest to train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokens_train, tokens_test, y_train, y_test = train_test_split(tokens, y, test_size=0.10)\n",
    "\n",
    "# fit_tranform is used for iterable of strings, it combines the fit and transform steps\n",
    "X_train = vectorizer.fit_transform(tokens_train)\n",
    "X_test = vectorizer.transform(tokens_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1798, 8866)\n",
      "(200, 8866)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Multinomial Naive Bayes Classifier has an accuracy of 95%. That's not bad. I could imagine sampling some sentences from a website, passing them to the model and choosing the language the model guesses the most. With 95% accuracy for each guess, sampling a few sentences should yeild the correct language. Of course, only having vocabulary from one wikipedia article probably means I'd see this accuracy go down if I tested it on a brand new article. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Nerual Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now I'll see if a simple Linear Neural Network performs any better or worse than the Naive Bayes classifier using the same BOW feature extraction. \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# create fully connected network\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# set device\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# convert sklearn sparse metrices to torch tensors\n",
    "\n",
    "# dense layer deals with float datatype\n",
    "X_train_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "X_test_tensor = torch.from_numpy(X_test.todense()).float()\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# hyperparameters\n",
    "\n",
    "vocab_size = X_train_tensor.shape[1]\n",
    "hidden_size = 4000\n",
    "num_classes = 8\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "\n",
    "# load data\n",
    "# TensorData creates a list of tuples with each record containing a BOW vector and a target language\n",
    "\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# initialize network\n",
    "\n",
    "model = NN(\n",
    "    vocab_size=vocab_size, \n",
    "    hidden_size=hidden_size, \n",
    "    num_classes=num_classes ).to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss() requires integer-encoded target\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train network\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# check accuracy on training & test to see how our model performs\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # evaluation mode\n",
    "\n",
    "    # don't have to compute gradient when checking the accuracy\n",
    "    with torch.no_grad(): \n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            # 64 x 8\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "\n",
    "    model.train() # return model to train\n",
    "\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Got 1797 / 1798 with accuracy 99.94\n",
      "Got 196 / 200 with accuracy 98.00\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Neural Network performs with an accuracy of 98% after three epochs! That's definitely an improvement from the Naive Bayes classifier. It's definitely more code, but I'd say it's worth it for the increased accuracy. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6bdf263dd1b1cef4309b77b00a11a6b470a762f164881b61a60ce80391d462c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('nlp_venv': venv)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}