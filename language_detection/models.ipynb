{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the data from Wikipedia, I'm going to try training a couple different models and see how each of them stack up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>language_code</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de</td>\n",
       "      <td>In stecken ebenfalls viele Anspielungen auf Al...</td>\n",
       "      <td>3</td>\n",
       "      <td>[stecken, ebenfalls, anspielung, alice, wunder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>The ballet returned to the Royal Opera House i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ballet, return, royal, opera, house, act, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es</td>\n",
       "      <td>El Grifo y la Falsa Tortuga escuchan hasta el ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[grifo, y, falsa, tortuga, escuchar, encuentro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ru</td>\n",
       "      <td>П.</td>\n",
       "      <td>4</td>\n",
       "      <td>[п.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>es</td>\n",
       "      <td>En el desarrollo del libro, Dodgson añadió dos...</td>\n",
       "      <td>2</td>\n",
       "      <td>[desarrollo, libro, dodgson, capítulo, ​ , ext...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                           sentence  language_code  \\\n",
       "0       de  In stecken ebenfalls viele Anspielungen auf Al...              3   \n",
       "1       en  The ballet returned to the Royal Opera House i...              0   \n",
       "2       es  El Grifo y la Falsa Tortuga escuchan hasta el ...              2   \n",
       "3       ru                                                 П.              4   \n",
       "4       es  En el desarrollo del libro, Dodgson añadió dos...              2   \n",
       "\n",
       "                                              tokens  \n",
       "0  [stecken, ebenfalls, anspielung, alice, wunder...  \n",
       "1  [ballet, return, royal, opera, house, act, sta...  \n",
       "2  [grifo, y, falsa, tortuga, escuchar, encuentro...  \n",
       "3                                               [п.]  \n",
       "4  [desarrollo, libro, dodgson, capítulo, ​ , ext...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "# read dataframe\r\n",
    "language_df = pd.read_pickle('language_data.pickle')\r\n",
    "\r\n",
    "# shuffle dataframe\r\n",
    "language_df = language_df.sample(frac=1).reset_index(drop=True)\r\n",
    "language_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangDetect Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before I train anything I want to see how accurate the langdetect library would be on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8556390977443609\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from langdetect import detect as ld\n",
    "\n",
    "def get_lang(text):\n",
    "    try:\n",
    "        return ld(text)\n",
    "    except:\n",
    "        return 'fail'\n",
    "\n",
    "accuracy_list = []\n",
    "lang_targets = language_df['language'].tolist()\n",
    "ld_preds = language_df['sentence'].apply(lambda x: get_lang(x)).tolist()\n",
    "for t, p in zip(lang_targets, ld_preds):\n",
    "    accuracy_list.append(t==p)\n",
    "\n",
    "counts = Counter(accuracy_list)\n",
    "print(\"Accuracy:\", counts[True]/len(accuracy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85 percent isn't bad; the accuracy may be affected by the quality of the data and not necessarily because of the langetect library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bow Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since this model's going to be trained to classify languages, sequence may not play as critical a role. I feel like the uniqueness of the words in each language should be enough for the model to learn how to make the correct distinctions. If that's the case than a BOW model should be enough, so I'll start by creating a count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1496, 7695)\n",
      "(499, 7695)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "y = language_df['language_code'].values\n",
    "# y = language_df['language_vector].tolist() # for one-hot-encoding\n",
    "tokens = language_df['tokens'].values\n",
    "\n",
    "tokens_train, tokens_test, y_train, y_test = train_test_split(tokens, y, test_size = 0.25, random_state=1000)\n",
    "\n",
    "# disable analyzer since I'm applying CountVectorizer to list of lemms\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "# fit_tranform is used for iterable of strings\n",
    "# it combines the fit and transform steps\n",
    "X_train = vectorizer.fit_transform(tokens_train)\n",
    "X_test = vectorizer.transform(tokens_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9438877755511023\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multinomial Naive Bayes Classifier has an accuracy of 94.4%. I think that's pretty good! I could imagine sampling some sentneces from a website, passing them to the model and choosing the language the model guesses the most. With 94.4% accuracy for each guess, sampling a few sentences should yeild the correct language. Of course, only having vocabulary from one wikipedia article probably means I'd see this accuracy go down if I tested it on a brand new article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Nerual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I'll see if a simple Linear Neural Network performs any better or worse than the Naive Bayes classifier using the same BOW vectors. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fully connected network\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# convert sklearn vectors to torch tensors\n",
    "# dense layer deals with float datatype\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train.todense()).float()\n",
    "X_test_tensor = torch.from_numpy(X_test.todense()).float()\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\r\n",
    "\r\n",
    "vocab_size = X_train_tensor.shape[1]\r\n",
    "hidden_size = 4000\r\n",
    "num_classes = 8\r\n",
    "learning_rate = 0.001\r\n",
    "batch_size = 32\r\n",
    "num_epochs = 3\r\n",
    "\r\n",
    "# load data\r\n",
    "# TensorData creates a list of tuples with each record containing a BOW vector and a target language\r\n",
    "\r\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\r\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\r\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\r\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize network\n",
    "\n",
    "model = NN(\n",
    "    vocab_size=vocab_size, \n",
    "    hidden_size=hidden_size, \n",
    "    num_classes=num_classes ).to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "\n",
    "# CrossEntropyLoss() requires integer-encoded target, not one-hot-encoded target\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train network\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1496 / 1496 with accuracy 100.00\n",
      "Got 471 / 499 with accuracy 94.39\n"
     ]
    }
   ],
   "source": [
    "# check accuracy on training & test to see how our model performs\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # evaluation mode\n",
    "\n",
    "    # don't have to compute gradient when checking the accuracy\n",
    "    with torch.no_grad(): \n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            # 64 x 8\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "\n",
    "    model.train() # return model to train\n",
    "\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Network performs about as well as the Naive Bayes classifier after three epochs. It's a lot more code, and it may be unnecessary for this task, but it's interesting to see how the neural net performs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integer Map Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I don't think any sort of semantic analysis would be necessary for this particular task. But just for the fun of it I want to see if a 1D convolutional neural network with word embeddings will perform any better or worse than the previous BOW models. This time instead of a count vectorizer I'll create integer map vectors using keras' preprocessing library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " word tokens: ['幼い', '追う', '不思議', '国', '迷う', '込む', 'しゃべる', '動物', '動く', 'さまざま', '出る', '会う', '世界', '冒険', 'さま', '描く']\n",
      "\n",
      " padded vectors: [2450 1305   34   27 2451  610 2452  217 2453 1306  264 1307   97  611\n",
      " 2454  612    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "\r\n",
    "tokenizer = Tokenizer(num_words=7500) # keep 7500 most common\r\n",
    "tokenizer.fit_on_texts(tokens_train)\r\n",
    "\r\n",
    "X_train = tokenizer.texts_to_sequences(tokens_train)\r\n",
    "X_test = tokenizer.texts_to_sequences(tokens_test)\r\n",
    "\r\n",
    "vocab_size = len(tokenizer.word_index) + 1 # Adding 1 because of reserved 0 index\r\n",
    "\r\n",
    "maxlen = 100 # this cuts sequences that exceed 100\r\n",
    "\r\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\r\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\r\n",
    "\r\n",
    "print('\\n', 'word tokens:', tokens_train[2])\r\n",
    "print('\\n', 'padded vectors:', X_train[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create simple CNN\r\n",
    "\r\n",
    "class CNN(nn.Module):\r\n",
    "    \"\"\"A 1D Convolutional Neural Network\"\"\"\r\n",
    "    def __init__(self, \r\n",
    "                 pretrained_embedding=None,\r\n",
    "                 freeze_embedding=False,\r\n",
    "                 vocab_size=None,\r\n",
    "                 embed_dim=128,\r\n",
    "                 filter_sizes=[3, 4, 5],\r\n",
    "                 num_filters=[100, 100, 100],\r\n",
    "                 num_classes=8,\r\n",
    "                 dropout=0.5):\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "    The constructor for CNN_NLP class.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        freeze_embedding (bool): Set to False to fine-tune pretraiend vectors.\r\n",
    "        embed_dim (int): Dimension of word vectors.\r\n",
    "        filter_sizes (List[int]): List of filter sizes.\r\n",
    "        num_filters (List[int]): List of number of filters, has the same length as filter_sizes.\r\n",
    "        n_classes (int): Number of classes.\r\n",
    "        dropout (float): Dropout rate.\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        super(CNN, self).__init__()\r\n",
    "        # Embedding layer\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\r\n",
    "                                      embedding_dim=self.embed_dim,\r\n",
    "                                      padding_idx=0,\r\n",
    "                                      max_norm=5.0)\r\n",
    "        # Conv Network\r\n",
    "        self.conv1d_list = nn.ModuleList([\r\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\r\n",
    "                      out_channels=num_filters[i],\r\n",
    "                      kernel_size=filter_sizes[i])\r\n",
    "            for i in range(len(filter_sizes))\r\n",
    "        ])\r\n",
    "        # Fully-connected layer and Dropout\r\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\r\n",
    "        self.dropout = nn.Dropout(p=dropout)\r\n",
    "\r\n",
    "    def forward(self, input_ids):\r\n",
    "        # Get embeddings from input_ids\r\n",
    "        # Output shape: (b, max_len, embed_dim)\r\n",
    "        x_embed = self.embedding(input_ids).float()\r\n",
    "\r\n",
    "        # Permute x_embed to match input shape requirement of nn.Conv1d\r\n",
    "        # Output shape: (b, embed_dim, max_len)\r\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\r\n",
    "\r\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\r\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) \r\n",
    "            for conv1d in self.conv1d_list]\r\n",
    "\r\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\r\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\r\n",
    "            for x_conv in x_conv_list]\r\n",
    "        \r\n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\r\n",
    "        # Output shape: (b, sum(num_filters))\r\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list], dim=1)\r\n",
    "        \r\n",
    "        # Compute logits. Output shape: (b, n_classes)\r\n",
    "        logits = self.fc(self.dropout(x_fc))\r\n",
    "\r\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\r\n",
    "\r\n",
    "device = torch.device('cpu')\r\n",
    "\r\n",
    "# convert train and test sets to tensors\r\n",
    "\r\n",
    "X_train_tensor = torch.tensor(X_train)\r\n",
    "X_test_tensor = torch.tensor(X_test)\r\n",
    "y_train_tensor = torch.from_numpy(y_train)\r\n",
    "y_test_tensor = torch.from_numpy(y_test)\r\n",
    "\r\n",
    "# hyperparameters\r\n",
    "\r\n",
    "num_classes = 8\r\n",
    "vocab_size = vocab_size\r\n",
    "batch_size = 32\r\n",
    "num_epochs = 3\r\n",
    "learning_rate = 0.01\r\n",
    "\r\n",
    "# load data\r\n",
    "\r\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\r\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\r\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\r\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network\r\n",
    "\r\n",
    "cnn_model = CNN(vocab_size=vocab_size, num_classes=num_classes).to(device)\r\n",
    "\r\n",
    "# loss and optimizer\r\n",
    "\r\n",
    "# CrossEntropyLoss() requires integer-encoded target, not one-hot-encoded target\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "# train network\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\r\n",
    "        data = data.to(device=device)\r\n",
    "        targets = targets.to(device=device)\r\n",
    "\r\n",
    "        # forward\r\n",
    "        logits = cnn_model(data)\r\n",
    "        loss = criterion(logits, targets)\r\n",
    "\r\n",
    "        # backward\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        # gradient descent or adam step\r\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1494 / 1496 with accuracy 99.87\n",
      "Got 456 / 499 with accuracy 91.38\n"
     ]
    }
   ],
   "source": [
    "# check accuracy on training & test to see how our model performs\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # evaluation mode\n",
    "\n",
    "    # don't have to compute gradient when checking the accuracy\n",
    "    with torch.no_grad(): \n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            logits = cnn_model(x)\n",
    "            _, predictions = logits.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "\n",
    "    model.train() # return model to train\n",
    "\n",
    "check_accuracy(train_loader, cnn_model)\n",
    "check_accuracy(test_loader, cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is a little less than the BOW Linear Neural Net. Like I mentioned earlier, I suspect the reason for the lower accuracy is that semantic relationships within each sentence don't matter as much as the uniqueness of words for language classification.\r\n",
    "\r\n",
    "### But 91.3 percent is still pretty good! "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6bdf263dd1b1cef4309b77b00a11a6b470a762f164881b61a60ce80391d462c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('nlp_venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}