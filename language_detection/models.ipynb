{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using the data from Wikipedia, I'm going to try training a couple different models and see how each of them perform. The goal is to see how accurately each model is at predicting the language of a given string. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "# read dataframe\r\n",
    "language_df = pd.read_pickle('language_data.pickle')\r\n",
    "\r\n",
    "# shuffle dataframe\r\n",
    "language_df = language_df.sample(frac=1).reset_index(drop=True)\r\n",
    "language_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  language                                           sentence  language_code  \\\n",
       "0       it  Le ragazze la adorarono e Alice Liddell chiese...              6   \n",
       "1       ru    В сказке впервые появились цветные иллюстрации.              4   \n",
       "2       ru  Первоначальный стих Саути, по мнению Хораса Гр...              4   \n",
       "3       ru  Вот их полный список в порядке появления в кни...              4   \n",
       "4       de  In Nickolas Cooks Adaption Alice in Zombieland...              3   \n",
       "\n",
       "                                              tokens  \n",
       "0  [ragazza, adorare, e, alice, liddell, chiesa, ...  \n",
       "1  [сказка, впервые, появиться, цветной, иллюстра...  \n",
       "2  [первоначальный, стих, саути, мнение, хораса, ...  \n",
       "3  [полный, список, порядок, появление, книга, им...  \n",
       "4  [nickolas, cooks, adaption, alice, zombieland,...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>language_code</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it</td>\n",
       "      <td>Le ragazze la adorarono e Alice Liddell chiese...</td>\n",
       "      <td>6</td>\n",
       "      <td>[ragazza, adorare, e, alice, liddell, chiesa, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ru</td>\n",
       "      <td>В сказке впервые появились цветные иллюстрации.</td>\n",
       "      <td>4</td>\n",
       "      <td>[сказка, впервые, появиться, цветной, иллюстра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ru</td>\n",
       "      <td>Первоначальный стих Саути, по мнению Хораса Гр...</td>\n",
       "      <td>4</td>\n",
       "      <td>[первоначальный, стих, саути, мнение, хораса, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ru</td>\n",
       "      <td>Вот их полный список в порядке появления в кни...</td>\n",
       "      <td>4</td>\n",
       "      <td>[полный, список, порядок, появление, книга, им...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de</td>\n",
       "      <td>In Nickolas Cooks Adaption Alice in Zombieland...</td>\n",
       "      <td>3</td>\n",
       "      <td>[nickolas, cooks, adaption, alice, zombieland,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LangDetect Library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Before I train anything I want to see how accurate the langdetect library would be on this data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from collections import Counter\r\n",
    "from langdetect import detect as ld\r\n",
    "\r\n",
    "def get_lang(text):\r\n",
    "    try:\r\n",
    "        return ld(text)\r\n",
    "    except:\r\n",
    "        return 'fail'\r\n",
    "\r\n",
    "accuracy_list = []\r\n",
    "lang_targets = language_df['language'].tolist()\r\n",
    "ld_preds = language_df['sentence'].apply(lambda x: get_lang(x)).tolist()\r\n",
    "for t, p in zip(lang_targets, ld_preds):\r\n",
    "    accuracy_list.append(t==p)\r\n",
    "\r\n",
    "counts = Counter(accuracy_list)\r\n",
    "print(\"Accuracy:\", counts[True]/len(accuracy_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.8568568568568569\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 86% isn't bad; the accuracy may be affected by the quality of the data and not necessarily because of the langetect library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BOW Feature Extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Since this model's going to be trained to classify languages, I suspect that sequence doesn't play a critical role. I think that the uniqueness of the words in each language should be enough for the model to learn how to make the correct distinctions. If that's the case then a BOW model should be enough, so I'll start by creating a count vectorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "\r\n",
    "tokens = language_df['tokens'].values\r\n",
    "y = language_df['language_code'].values\r\n",
    "#y = language_df['language_vector].tolist() # for one-hot-encoding\r\n",
    "\r\n",
    "tokens_train, tokens_test, y_train, y_test = train_test_split(tokens, y, test_size = 0.25, random_state=1000)\r\n",
    "\r\n",
    "# disable analyzer since I'm applying CountVectorizer to list of lemms\r\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x)\r\n",
    "\r\n",
    "# fit_tranform is used for iterable of strings, it combines the fit and transform steps\r\n",
    "X_train = vectorizer.fit_transform(tokens_train)\r\n",
    "X_test = vectorizer.transform(tokens_test)\r\n",
    "\r\n",
    "print(X_train.shape)\r\n",
    "print(X_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1498, 7986)\n",
      "(500, 7986)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Naive Bayes MultinomialNB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn import metrics\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "\r\n",
    "clf = MultinomialNB()\r\n",
    "clf.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_pred = clf.predict(X_test)\r\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.956\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Multinomial Naive Bayes Classifier has an accuracy of 95.6%. I think that's pretty good! I could imagine sampling some sentneces from a website, passing them to the model and choosing the language the model guesses the most. With 95.6% accuracy for each guess, sampling a few sentences should yeild the correct language. Of course, only having vocabulary from one wikipedia article probably means I'd see this accuracy go down if I tested it on a brand new article. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Nerual Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now I'll see if a simple Linear Neural Network performs any better or worse than the Naive Bayes classifier using the same BOW vectors. \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# create fully connected network\r\n",
    "\r\n",
    "class NN(nn.Module):\r\n",
    "    def __init__(self, vocab_size, hidden_size, num_classes):\r\n",
    "        super(NN, self).__init__()\r\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_size)\r\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        x = self.fc2(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# set device\r\n",
    "\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "\r\n",
    "# convert sklearn vectors to torch tensors\r\n",
    "# dense layer deals with float datatype\r\n",
    "\r\n",
    "X_train_tensor = torch.from_numpy(X_train.todense()).float()\r\n",
    "X_test_tensor = torch.from_numpy(X_test.todense()).float()\r\n",
    "y_train_tensor = torch.from_numpy(y_train)\r\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# hyperparameters\r\n",
    "\r\n",
    "vocab_size = X_train_tensor.shape[1]\r\n",
    "hidden_size = 4000\r\n",
    "num_classes = 8\r\n",
    "learning_rate = 0.001\r\n",
    "batch_size = 32\r\n",
    "num_epochs = 3\r\n",
    "\r\n",
    "# load data\r\n",
    "# TensorData creates a list of tuples with each record containing a BOW vector and a target language\r\n",
    "\r\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\r\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\r\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\r\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# initialize network\r\n",
    "\r\n",
    "model = NN(\r\n",
    "    vocab_size=vocab_size, \r\n",
    "    hidden_size=hidden_size, \r\n",
    "    num_classes=num_classes ).to(device)\r\n",
    "\r\n",
    "# loss and optimizer\r\n",
    "\r\n",
    "# CrossEntropyLoss() requires integer-encoded target\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "# train network\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\r\n",
    "        data = data.to(device=device)\r\n",
    "        targets = targets.to(device=device)\r\n",
    "\r\n",
    "        # forward\r\n",
    "        scores = model(data)\r\n",
    "        loss = criterion(scores, targets)\r\n",
    "\r\n",
    "        # backward\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        # gradient descent or adam step\r\n",
    "        optimizer.step()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# check accuracy on training & test to see how our model performs\r\n",
    "\r\n",
    "def check_accuracy(loader, model):\r\n",
    "    num_correct = 0\r\n",
    "    num_samples = 0\r\n",
    "    model.eval() # evaluation mode\r\n",
    "\r\n",
    "    # don't have to compute gradient when checking the accuracy\r\n",
    "    with torch.no_grad(): \r\n",
    "        for x, y in loader:\r\n",
    "            x = x.to(device=device)\r\n",
    "            y = y.to(device=device)\r\n",
    "\r\n",
    "            scores = model(x)\r\n",
    "            # 64 x 8\r\n",
    "            _, predictions = scores.max(1)\r\n",
    "            num_correct += (predictions == y).sum()\r\n",
    "            num_samples += predictions.size(0)\r\n",
    "\r\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\r\n",
    "\r\n",
    "    model.train() # return model to train\r\n",
    "\r\n",
    "check_accuracy(train_loader, model)\r\n",
    "check_accuracy(test_loader, model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Got 1497 / 1498 with accuracy 99.93\n",
      "Got 476 / 500 with accuracy 95.20\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Neural Network performs about as well as the Naive Bayes classifier after three epochs. It's a lot more code, and it may be unnecessary for this task, but it's interesting to see how the neural net performs!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Integer Map Feature Extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I don't think any sort of semantic analysis would be necessary for this particular task. But just for the fun of it I want to see if a 1D convolutional neural network with word embeddings will perform any better or worse than the previous BOW models. This time instead of a count vectorizer I'll create integer map vectors using keras' preprocessing library. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "\r\n",
    "tokenizer = Tokenizer(num_words=7500) # keep 7500 most common\r\n",
    "tokenizer.fit_on_texts(tokens_train)\r\n",
    "\r\n",
    "X_train = tokenizer.texts_to_sequences(tokens_train)\r\n",
    "X_test = tokenizer.texts_to_sequences(tokens_test)\r\n",
    "\r\n",
    "vocab_size = len(tokenizer.word_index) + 1 # Adding 1 because of reserved 0 index\r\n",
    "\r\n",
    "maxlen = 100 # this cuts sequences that exceed 100\r\n",
    "\r\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\r\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\r\n",
    "\r\n",
    "print('\\n', 'word tokens:', tokens_train[2])\r\n",
    "print('\\n', 'padded vectors:', X_train[2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " word tokens: ['песня', 'морской', 'кадриль', 'поёт', 'черепаха', 'квази', 'пародировать', 'стихотворение', 'мэри', 'хауитт', 'паук', 'муха']\n",
      "\n",
      " padded vectors: [ 247  375  292  881   96  206  476   55  248 2564 2565 2566    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# create simple CNN\r\n",
    "\r\n",
    "class CNN(nn.Module):\r\n",
    "    \"\"\"A 1D Convolutional Neural Network\"\"\"\r\n",
    "    def __init__(self, \r\n",
    "                 pretrained_embedding=None,\r\n",
    "                 freeze_embedding=False,\r\n",
    "                 vocab_size=None,\r\n",
    "                 embed_dim=128,\r\n",
    "                 filter_sizes=[3, 4, 5],\r\n",
    "                 num_filters=[100, 100, 100],\r\n",
    "                 num_classes=8,\r\n",
    "                 dropout=0.5):\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "    The constructor for CNN_NLP class.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        freeze_embedding (bool): Set to False to fine-tune pretraiend vectors.\r\n",
    "        embed_dim (int): Dimension of word vectors.\r\n",
    "        filter_sizes (List[int]): List of filter sizes.\r\n",
    "        num_filters (List[int]): List of number of filters, has the same length as filter_sizes.\r\n",
    "        n_classes (int): Number of classes.\r\n",
    "        dropout (float): Dropout rate.\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        super(CNN, self).__init__()\r\n",
    "        # Embedding layer\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\r\n",
    "                                      embedding_dim=self.embed_dim,\r\n",
    "                                      padding_idx=0,\r\n",
    "                                      max_norm=5.0)\r\n",
    "        # Conv Network\r\n",
    "        self.conv1d_list = nn.ModuleList([\r\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\r\n",
    "                      out_channels=num_filters[i],\r\n",
    "                      kernel_size=filter_sizes[i])\r\n",
    "            for i in range(len(filter_sizes))\r\n",
    "        ])\r\n",
    "        # Fully-connected layer and Dropout\r\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\r\n",
    "        self.dropout = nn.Dropout(p=dropout)\r\n",
    "\r\n",
    "    def forward(self, input_ids):\r\n",
    "        # Get embeddings from input_ids\r\n",
    "        # Output shape: (b, max_len, embed_dim)\r\n",
    "        x_embed = self.embedding(input_ids).float()\r\n",
    "\r\n",
    "        # Permute x_embed to match input shape requirement of nn.Conv1d\r\n",
    "        # Output shape: (b, embed_dim, max_len)\r\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\r\n",
    "\r\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\r\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) \r\n",
    "            for conv1d in self.conv1d_list]\r\n",
    "\r\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\r\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\r\n",
    "            for x_conv in x_conv_list]\r\n",
    "        \r\n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\r\n",
    "        # Output shape: (b, sum(num_filters))\r\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list], dim=1)\r\n",
    "        \r\n",
    "        # Compute logits. Output shape: (b, n_classes)\r\n",
    "        logits = self.fc(self.dropout(x_fc))\r\n",
    "\r\n",
    "        return logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# set device\r\n",
    "\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "\r\n",
    "# convert train and test sets to tensors\r\n",
    "\r\n",
    "X_train_tensor = torch.tensor(X_train)\r\n",
    "X_test_tensor = torch.tensor(X_test)\r\n",
    "y_train_tensor = torch.from_numpy(y_train)\r\n",
    "y_test_tensor = torch.from_numpy(y_test)\r\n",
    "\r\n",
    "# hyperparameters\r\n",
    "\r\n",
    "num_classes = 8\r\n",
    "vocab_size = vocab_size\r\n",
    "batch_size = 32\r\n",
    "num_epochs = 3\r\n",
    "learning_rate = 0.01\r\n",
    "\r\n",
    "# load data\r\n",
    "\r\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\r\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\r\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\r\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# initialize network\r\n",
    "\r\n",
    "cnn_model = CNN(vocab_size=vocab_size, num_classes=num_classes).to(device)\r\n",
    "\r\n",
    "# loss and optimizer\r\n",
    "\r\n",
    "# CrossEntropyLoss() requires integer-encoded target, not one-hot-encoded target\r\n",
    "\r\n",
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=learning_rate)\r\n",
    "\r\n",
    "# train network\r\n",
    "\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\r\n",
    "        data = data.to(device=device)\r\n",
    "        targets = targets.to(device=device)\r\n",
    "\r\n",
    "        # forward\r\n",
    "        logits = cnn_model(data)\r\n",
    "        loss = criterion(logits, targets)\r\n",
    "\r\n",
    "        # backward\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "\r\n",
    "        # gradient descent or adam step\r\n",
    "        optimizer.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# check accuracy on training & test to see how our model performs\r\n",
    "\r\n",
    "def check_accuracy(loader, model):\r\n",
    "    num_correct = 0\r\n",
    "    num_samples = 0\r\n",
    "    model.eval() # evaluation mode\r\n",
    "\r\n",
    "    # don't have to compute gradient when checking the accuracy\r\n",
    "    with torch.no_grad(): \r\n",
    "        for x, y in loader:\r\n",
    "            x = x.to(device=device)\r\n",
    "            y = y.to(device=device)\r\n",
    "\r\n",
    "            logits = cnn_model(x)\r\n",
    "            _, predictions = logits.max(1)\r\n",
    "            num_correct += (predictions == y).sum()\r\n",
    "            num_samples += predictions.size(0)\r\n",
    "\r\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\r\n",
    "\r\n",
    "    model.train() # return model to train\r\n",
    "\r\n",
    "check_accuracy(train_loader, cnn_model)\r\n",
    "check_accuracy(test_loader, cnn_model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Got 1497 / 1498 with accuracy 99.93\n",
      "Got 460 / 500 with accuracy 92.00\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This model's accuracy is a little less than the BOW Linear Neural Net. Like I mentioned earlier, I suspect the reason for the lower accuracy is that semantic relationships within each sentence don't matter as much as the uniqueness of words for language classification.\r\n",
    "\r\n",
    "### But 92% is still pretty good! \r\n",
    "\r\n",
    "### I decided to try implementing my own language detection models in response to an interview question I was asked recently, and I wanted to test if the ideas I gave during that interview could actually work. The question was how I would go about detecting which language a website is written in. I thought the best way to go about it would be to use a model like the ones I trained in this notebook and feed it a sample of processed sents from the website in question. This might not be the best way to accomplish this task, but it's given me a good excuse to practice implementing different types of neural networks in pytorch. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6bdf263dd1b1cef4309b77b00a11a6b470a762f164881b61a60ce80391d462c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('nlp_venv': venv)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}